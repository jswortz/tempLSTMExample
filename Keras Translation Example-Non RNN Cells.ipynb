{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 69\n",
      "Number of unique output tokens: 93\n",
      "Max sequence length for inputs: 16\n",
      "Max sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "import numpy as np\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from tensorflow.python import keras\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "'''Sequence to sequence example in Keras (character-level).\n",
    "This script demonstrates how to implement a basic character-level\n",
    "sequence-to-sequence model. We apply it to translating\n",
    "short English sentences into short French sentences,\n",
    "character-by-character. Note that it is fairly unusual to\n",
    "do character-level machine translation, as word-level\n",
    "models are more common in this domain.\n",
    "# Summary of the algorithm\n",
    "- We start with input sequences from a domain (e.g. English sentences)\n",
    "    and corresponding target sequences from another domain\n",
    "    (e.g. French sentences).\n",
    "- An encoder LSTM turns input sequences to 2 state vectors\n",
    "    (we keep the last LSTM state and discard the outputs).\n",
    "- A decoder LSTM is trained to turn the target sequences into\n",
    "    the same sequence but offset by one timestep in the future,\n",
    "    a training process called \"teacher forcing\" in this context.\n",
    "    It uses as initial state the state vectors from the encoder.\n",
    "    Effectively, the decoder learns to generate `targets[t+1...]`\n",
    "    given `targets[...t]`, conditioned on the input sequence.\n",
    "- In inference mode, when we want to decode unknown input sequences, we:\n",
    "    - Encode the input sequence into state vectors\n",
    "    - Start with a target sequence of size 1\n",
    "        (just the start-of-sequence character)\n",
    "    - Feed the state vectors and 1-char target sequence\n",
    "        to the decoder to produce predictions for the next character\n",
    "    - Sample the next character using these predictions\n",
    "        (we simply use argmax).\n",
    "    - Append the sampled character to the target sequence\n",
    "    - Repeat until we generate the end-of-sequence character or we\n",
    "        hit the character limit.\n",
    "# Data download\n",
    "English to French sentence pairs.\n",
    "http://www.manythings.org/anki/fra-eng.zip\n",
    "Lots of neat sentence pairs datasets can be found at:\n",
    "http://www.manythings.org/anki/\n",
    "# References\n",
    "- Sequence to Sequence Learning with Neural Networks\n",
    "    https://arxiv.org/abs/1409.3215\n",
    "- Learning Phrase Representations using\n",
    "    RNN Encoder-Decoder for Statistical Machine Translation\n",
    "    https://arxiv.org/abs/1406.1078\n",
    "'''\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Batch size for training.\n",
    "epochs = 100  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000  # Number of samples to train on.\n",
    "# Path to the data txt file on disk.\n",
    "data_path = 'fra-eng/fra.txt'\n",
    "\n",
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype ='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updatable plot\n",
    "# a minimal example (sort of)\n",
    "import keras\n",
    "class PlotLosses(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.i = 0\n",
    "        self.x = []\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.mse = []\n",
    "        self.mae = []\n",
    "        \n",
    "        self.fig = plt.figure()\n",
    "        \n",
    "        self.logs = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \n",
    "        self.logs.append(logs)\n",
    "        self.x.append(self.i)\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.val_losses.append(logs.get('val_loss'))\n",
    "        self.accuracies.append(logs.get('acc'))\n",
    "        self.val_accuracies.append(logs.get('val_acc'))\n",
    "        self.i += 1\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.x, self.losses, label=\"loss\")\n",
    "        plt.plot(self.x, self.val_losses, label=\"val_loss\")\n",
    "        plt.plot(self.x, self.accuracies, label=\"acc\")\n",
    "        plt.plot(self.x, self.val_accuracies, label=\"val_acc\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.rcParams[\"figure.figsize\"] = [16,9]\n",
    "        \n",
    "plot_losses = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_43 (InputLayer)           (None, None, 69)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_44 (InputLayer)           (None, None, 93)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_29 (LSTM)                  [(None, None, 256),  333824      input_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  [(None, None, 256),  358400      input_44[0][0]                   \n",
      "                                                                 lstm_29[0][1]                    \n",
      "                                                                 lstm_29[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_30 (LSTM)                  [(None, 256), (None, 525312      lstm_29[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  [(None, None, 256),  525312      lstm_31[0][0]                    \n",
      "                                                                 lstm_30[0][1]                    \n",
      "                                                                 lstm_30[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, None, 93)     23901       lstm_32[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,766,749\n",
      "Trainable params: 1,766,749\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, RNN\n",
    "layers = [256,128] # we loop LSTMCells then wrap them in an RNN layer\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "e_outputs, h1, c1 = LSTM(latent_dim, return_state=True, return_sequences=True)(encoder_inputs) \n",
    "_, h2, c2 = LSTM(latent_dim, return_state=True)(e_outputs) \n",
    "encoder_states = [h1, c1, h2, c2]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "out_layer1 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "d_outputs, dh1, dc1 = out_layer1(decoder_inputs,initial_state= [h1, c1])\n",
    "out_layer2 = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "final, dh2, dc2 = out_layer2(d_outputs, initial_state= [h2, c2])\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(final)\n",
    "\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 46s 6ms/step - loss: 0.9346 - acc: 0.0604 - val_loss: 0.9894 - val_acc: 0.0773\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'PlotLosses' object has no attribute 'accuracies'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-63500353bdc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m          callbacks=[plot_losses])\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's2s.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ThreeSix/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/ThreeSix/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ThreeSix/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-24947eefba9a>\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'PlotLosses' object has no attribute 'accuracies'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = 100\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', \n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2,\n",
    "         callbacks=[plot_losses])\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_34 (InputLayer)           (None, None, 93)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_39 (InputLayer)           (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_40 (InputLayer)           (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_27 (LSTM)                  [(None, None, 256),  358400      input_34[0][0]                   \n",
      "                                                                 input_39[0][0]                   \n",
      "                                                                 input_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_41 (InputLayer)           (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_28 (LSTM)                  [(None, None, 256),  525312      lstm_27[2][0]                    \n",
      "                                                                 input_41[0][0]                   \n",
      "                                                                 input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, None, 93)     23901       lstm_28[2][0]                    \n",
      "==================================================================================================\n",
      "Total params: 907,613\n",
      "Trainable params: 907,613\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_state_input_h1 = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c1 = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c, \n",
    "                         decoder_state_input_h1, decoder_state_input_c1]\n",
    "d_o, state_h, state_c = out_layer1(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs[:2])\n",
    "d_o, state_h1, state_c1 = out_layer2(\n",
    "    d_o, initial_state=decoder_states_inputs[-2:])\n",
    "decoder_states = [state_h, state_c, state_h1, state_c1]\n",
    "decoder_outputs = decoder_dense(d_o)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Go.\n",
      "Target sentence: \tVa !\n",
      "\n",
      "Decoded sentence: Allez !\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Target sentence: \tSalut !\n",
      "\n",
      "Decoded sentence: Salut !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Target sentence: \tCours !\n",
      "\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Run!\n",
      "Target sentence: \tCourez !\n",
      "\n",
      "Decoded sentence: Cours !\n",
      "\n",
      "-\n",
      "Input sentence: Wow!\n",
      "Target sentence: \tÇa alors !\n",
      "\n",
      "Decoded sentence: Ça alors !\n",
      "\n",
      "-\n",
      "Input sentence: Fire!\n",
      "Target sentence: \tAu feu !\n",
      "\n",
      "Decoded sentence: Au ferme !\n",
      "\n",
      "-\n",
      "Input sentence: Help!\n",
      "Target sentence: \tÀ l'aide !\n",
      "\n",
      "Decoded sentence: À l'aide !\n",
      "\n",
      "-\n",
      "Input sentence: Jump.\n",
      "Target sentence: \tSaute.\n",
      "\n",
      "Decoded sentence: Saute.\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Target sentence: \tÇa suffit !\n",
      "\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Target sentence: \tStop !\n",
      "\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Stop!\n",
      "Target sentence: \tArrête-toi !\n",
      "\n",
      "Decoded sentence: Arrête-toi !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Target sentence: \tAttends !\n",
      "\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Target sentence: \tAttendez !\n",
      "\n",
      "Decoded sentence: Attends !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Target sentence: \tPoursuis.\n",
      "\n",
      "Decoded sentence: Poursuis !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Target sentence: \tContinuez.\n",
      "\n",
      "Decoded sentence: Poursuis !\n",
      "\n",
      "-\n",
      "Input sentence: Go on.\n",
      "Target sentence: \tPoursuivez.\n",
      "\n",
      "Decoded sentence: Poursuis !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Target sentence: \tBonjour !\n",
      "\n",
      "Decoded sentence: Bonjour !\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Target sentence: \tSalut !\n",
      "\n",
      "Decoded sentence: Bonjour !\n",
      "\n",
      "-\n",
      "Input sentence: I see.\n",
      "Target sentence: \tJe comprends.\n",
      "\n",
      "Decoded sentence: Je comprends.\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Target sentence: \tJ'essaye.\n",
      "\n",
      "Decoded sentence: J'essaye.\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Target sentence: \tJ'ai gagné !\n",
      "\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Target sentence: \tJe l'ai emporté !\n",
      "\n",
      "Decoded sentence: J'ai gagné !\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Target sentence: \tOh non !\n",
      "\n",
      "Decoded sentence: Oh non !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Target sentence: \tAttaque !\n",
      "\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Attack!\n",
      "Target sentence: \tAttaquez !\n",
      "\n",
      "Decoded sentence: Attaque !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Target sentence: \tSanté !\n",
      "\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Target sentence: \tÀ votre santé !\n",
      "\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Target sentence: \tMerci !\n",
      "\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Target sentence: \tTchin-tchin !\n",
      "\n",
      "Decoded sentence: Santé !\n",
      "\n",
      "-\n",
      "Input sentence: Get up.\n",
      "Target sentence: \tLève-toi.\n",
      "\n",
      "Decoded sentence: Lavez-le !\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Target sentence: \tVa, maintenant.\n",
      "\n",
      "Decoded sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Target sentence: \tAllez-y maintenant.\n",
      "\n",
      "Decoded sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Go now.\n",
      "Target sentence: \tVas-y maintenant.\n",
      "\n",
      "Decoded sentence: Allez-y maintenant.\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Target sentence: \tJ'ai pigé !\n",
      "\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it!\n",
      "Target sentence: \tCompris !\n",
      "\n",
      "Decoded sentence: Compris !\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Target sentence: \tPigé ?\n",
      "\n",
      "Decoded sentence: T'as capté ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Target sentence: \tCompris ?\n",
      "\n",
      "Decoded sentence: T'as capté ?\n",
      "\n",
      "-\n",
      "Input sentence: Got it?\n",
      "Target sentence: \tT'as capté ?\n",
      "\n",
      "Decoded sentence: T'as capté ?\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Target sentence: \tMonte.\n",
      "\n",
      "Decoded sentence: Monte.\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Target sentence: \tMontez.\n",
      "\n",
      "Decoded sentence: Monte.\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Target sentence: \tSerre-moi dans tes bras !\n",
      "\n",
      "Decoded sentence: Serrez-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: Hug me.\n",
      "Target sentence: \tSerrez-moi dans vos bras !\n",
      "\n",
      "Decoded sentence: Serrez-moi dans tes bras !\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Target sentence: \tJe suis tombée.\n",
      "\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I fell.\n",
      "Target sentence: \tJe suis tombé.\n",
      "\n",
      "Decoded sentence: Je suis tombée.\n",
      "\n",
      "-\n",
      "Input sentence: I know.\n",
      "Target sentence: \tJe sais.\n",
      "\n",
      "Decoded sentence: Je sais.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Target sentence: \tJe suis parti.\n",
      "\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I left.\n",
      "Target sentence: \tJe suis partie.\n",
      "\n",
      "Decoded sentence: Je suis partie.\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Target sentence: \tJ'ai perdu.\n",
      "\n",
      "Decoded sentence: J'ai perdu.\n",
      "\n",
      "-\n",
      "Input sentence: I'm 19.\n",
      "Target sentence: \tJ'ai 19 ans.\n",
      "\n",
      "Decoded sentence: J'ai 19 ans.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Target sentence: \tJe vais bien.\n",
      "\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Target sentence: \tÇa va.\n",
      "\n",
      "Decoded sentence: Je vais bien.\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Target sentence: \tÉcoutez !\n",
      "\n",
      "Decoded sentence: Écoutez !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tC'est pas possible !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tImpossible !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tEn aucun cas.\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tSans façons !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tC'est hors de question !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tIl n'en est pas question !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tC'est exclu !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tEn aucune manière !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Target sentence: \tHors de question !\n",
      "\n",
      "Decoded sentence: C'est pas possible !\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Target sentence: \tVraiment ?\n",
      "\n",
      "Decoded sentence: Ah bon ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Target sentence: \tVrai ?\n",
      "\n",
      "Decoded sentence: Ah bon ?\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Target sentence: \tAh bon ?\n",
      "\n",
      "Decoded sentence: Ah bon ?\n",
      "\n",
      "-\n",
      "Input sentence: Thanks.\n",
      "Target sentence: \tMerci !\n",
      "\n",
      "Decoded sentence: Merci !\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Target sentence: \tOn essaye.\n",
      "\n",
      "Decoded sentence: On essaye.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Target sentence: \tNous avons gagné.\n",
      "\n",
      "Decoded sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Target sentence: \tNous gagnâmes.\n",
      "\n",
      "Decoded sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Target sentence: \tNous l'avons emporté.\n",
      "\n",
      "Decoded sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence: We won.\n",
      "Target sentence: \tNous l'emportâmes.\n",
      "\n",
      "Decoded sentence: Nous l'emportâmes.\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Target sentence: \tDemande à Tom.\n",
      "\n",
      "Decoded sentence: Demande à Tom.\n",
      "\n",
      "-\n",
      "Input sentence: Awesome!\n",
      "Target sentence: \tFantastique !\n",
      "\n",
      "Decoded sentence: Fantastique !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Target sentence: \tSois calme !\n",
      "\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Target sentence: \tSoyez calme !\n",
      "\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Target sentence: \tSoyez calmes !\n",
      "\n",
      "Decoded sentence: Soyez calmes !\n",
      "\n",
      "-\n",
      "Input sentence: Be cool.\n",
      "Target sentence: \tSois détendu !\n",
      "\n",
      "Decoded sentence: Sois détendu !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Target sentence: \tSois juste !\n",
      "\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Target sentence: \tSoyez juste !\n",
      "\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Target sentence: \tSoyez justes !\n",
      "\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Target sentence: \tSois équitable !\n",
      "\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Target sentence: \tSoyez équitable !\n",
      "\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Target sentence: \tSoyez équitables !\n",
      "\n",
      "Decoded sentence: Soyez équitables !\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Target sentence: \tSois gentil.\n",
      "\n",
      "Decoded sentence: Sois gentil.\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Target sentence: \tSois gentil !\n",
      "\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Target sentence: \tSois gentille !\n",
      "\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Target sentence: \tSoyez gentil !\n",
      "\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Target sentence: \tSoyez gentille !\n",
      "\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Target sentence: \tSoyez gentils !\n",
      "\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Target sentence: \tSoyez gentilles !\n",
      "\n",
      "Decoded sentence: Soyez gentilles !\n",
      "\n",
      "-\n",
      "Input sentence: Beat it.\n",
      "Target sentence: \tDégage !\n",
      "\n",
      "Decoded sentence: Dégage !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Target sentence: \tAppelle-moi !\n",
      "\n",
      "Decoded sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Target sentence: \tAppellez-moi !\n",
      "\n",
      "Decoded sentence: Appellez-moi !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Target sentence: \tAppelle-nous !\n",
      "\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Target sentence: \tAppelez-nous !\n",
      "\n",
      "Decoded sentence: Appelle-nous !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Target sentence: \tEntrez !\n",
      "\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Target sentence: \tEntre.\n",
      "\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Target sentence: \tEntre !\n",
      "\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Target sentence: \tEntrez !\n",
      "\n",
      "Decoded sentence: Entrez !\n",
      "\n",
      "-\n",
      "Input sentence: Come on!\n",
      "Target sentence: \tAllez !\n",
      "\n",
      "Decoded sentence: Allez !\n",
      "\n",
      "-\n",
      "Input sentence: Come on.\n",
      "Target sentence: \tAllez !\n",
      "\n",
      "Decoded sentence: Allez !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c, h1, c1 = decoder_model.predict(\n",
    "            [target_seq] + states_value) #######NOTICE THE ADDITIONAL HIDDEN STATES\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c, h1, c1]#######NOTICE THE ADDITIONAL HIDDEN STATES\n",
    "\n",
    "    return decoded_sentence\n",
    "\n",
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Target sentence:', target_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Run!\n",
      "Target sentence: \tCours !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seq_index = 2\n",
    "input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "print('Input sentence:', input_texts[seq_index])\n",
    "print('Target sentence:', target_texts[seq_index])\n",
    "input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "states_value = encoder_model.predict(input_seq)\n",
    "decoded_sentence = ''\n",
    "stop_condition = False\n",
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "target_seq\n",
    "decoded_sentence = ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens, h, c, h1, c1 = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "if (sampled_char == '\\n' or\n",
    "    len(decoded_sentence) > max_decoder_seq_length):\n",
    "        stop_condition = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[3.28468445e-07, 2.67626910e-06, 5.28903753e-02, 3.11445656e-05,\n",
       "         2.38574557e-06, 7.30980764e-06, 2.00140357e-06, 5.43985261e-05,\n",
       "         5.32724926e-07, 1.17922002e-06, 8.75312649e-03, 1.54463705e-02,\n",
       "         2.54699171e-01, 1.52343946e-05, 5.33733055e-06, 1.13827491e-06,\n",
       "         1.85874012e-06, 2.19590765e-06, 1.19934057e-05, 7.73286138e-06,\n",
       "         1.29239445e-04, 3.07484479e-05, 3.61205421e-05, 6.93354668e-05,\n",
       "         8.14259984e-05, 1.44352845e-04, 9.13643598e-05, 1.44697900e-04,\n",
       "         6.20983483e-05, 7.16865528e-04, 1.00129895e-04, 4.37703238e-06,\n",
       "         1.96771667e-04, 2.10526676e-04, 7.49556493e-05, 9.77450181e-05,\n",
       "         1.59461619e-04, 7.86551682e-05, 3.80623853e-04, 9.00865689e-06,\n",
       "         6.61972490e-06, 3.46562447e-05, 2.70279706e-05, 3.16538535e-05,\n",
       "         2.43877973e-02, 3.86256637e-04, 2.88435421e-03, 5.91463223e-03,\n",
       "         3.60161841e-01, 2.01036502e-03, 2.34382343e-03, 8.73518875e-04,\n",
       "         1.11173624e-02, 1.02480364e-04, 1.09566783e-04, 4.98931250e-03,\n",
       "         5.23052222e-05, 4.80143179e-04, 2.21981853e-03, 1.00158162e-04,\n",
       "         2.27144058e-03, 1.23819513e-02, 1.03732713e-01, 1.17715523e-02,\n",
       "         2.35231128e-04, 9.29103349e-04, 3.68742491e-07, 3.91749520e-04,\n",
       "         1.07190368e-04, 3.73811985e-04, 9.53667890e-03, 8.02627710e-06,\n",
       "         1.01128762e-05, 5.59879300e-05, 4.65852936e-05, 4.09725071e-05,\n",
       "         8.21097128e-05, 8.01826827e-04, 1.77875481e-04, 4.00934252e-04,\n",
       "         2.27098796e-03, 6.23550639e-02, 3.03936831e-04, 3.74031771e-07,\n",
       "         4.14815964e-04, 2.40932022e-05, 5.76978724e-04, 6.19872617e-06,\n",
       "         2.82446308e-05, 1.87012229e-05, 8.87443603e-04, 4.79532682e-05,\n",
       "         3.67202796e-02]]], dtype=float32)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "states_value = [c, h, c1, h1]\n",
    "sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "decoded_sentence += sampled_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aire'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
